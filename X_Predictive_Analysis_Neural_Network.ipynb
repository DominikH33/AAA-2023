{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a fitting NN and compare its accuracy and computational time with our SVM results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da mein Model einfach sehr fragw√ºrdige Ergebnisse liefert wird es gescrapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dompp\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with a simple dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first work on our hourly aggregated data without using geodata just to test the influence of certain prediction parameters and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_amount</th>\n",
       "      <th>mean_trip_seconds</th>\n",
       "      <th>mean_trip_miles</th>\n",
       "      <th>mean_trip_total</th>\n",
       "      <th>start_temp</th>\n",
       "      <th>start_precip</th>\n",
       "      <th>start_windspeed</th>\n",
       "      <th>end_temp</th>\n",
       "      <th>end_precip</th>\n",
       "      <th>end_windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>82</td>\n",
       "      <td>872.573171</td>\n",
       "      <td>4.837927</td>\n",
       "      <td>19.556829</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.35</td>\n",
       "      <td>-1.319024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.519024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>51</td>\n",
       "      <td>934.078431</td>\n",
       "      <td>5.023529</td>\n",
       "      <td>17.980392</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.12</td>\n",
       "      <td>-1.285882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.190588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 02:00:00</td>\n",
       "      <td>53</td>\n",
       "      <td>763.509434</td>\n",
       "      <td>4.466415</td>\n",
       "      <td>16.875283</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.48</td>\n",
       "      <td>-1.281698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.446038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01 03:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>773.105263</td>\n",
       "      <td>4.000526</td>\n",
       "      <td>17.217368</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>-1.136316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.303947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01 04:00:00</td>\n",
       "      <td>29</td>\n",
       "      <td>903.655172</td>\n",
       "      <td>3.885517</td>\n",
       "      <td>19.018621</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-0.906207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.615172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_start_timestamp  trip_amount  mean_trip_seconds  mean_trip_miles  \\\n",
       "0  2021-01-01 00:00:00           82         872.573171         4.837927   \n",
       "1  2021-01-01 01:00:00           51         934.078431         5.023529   \n",
       "2  2021-01-01 02:00:00           53         763.509434         4.466415   \n",
       "3  2021-01-01 03:00:00           38         773.105263         4.000526   \n",
       "4  2021-01-01 04:00:00           29         903.655172         3.885517   \n",
       "\n",
       "   mean_trip_total  start_temp  start_precip  start_windspeed  end_temp  \\\n",
       "0        19.556829       -1.33           0.0             6.35 -1.319024   \n",
       "1        17.980392       -1.28           0.0             7.12 -1.285882   \n",
       "2        16.875283       -1.31           0.0             7.48 -1.281698   \n",
       "3        17.217368       -1.16           0.0             7.30 -1.136316   \n",
       "4        19.018621       -0.98           0.0             7.33 -0.906207   \n",
       "\n",
       "   end_precip  end_windspeed  \n",
       "0         0.0       6.519024  \n",
       "1         0.0       7.190588  \n",
       "2         0.0       7.446038  \n",
       "3         0.0       7.303947  \n",
       "4         0.0       7.615172  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/\"\n",
    "taxi = pd.read_csv(f\"{file_path}taxi_hourly_processed.csv\")\n",
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since end_temp, end_precip, end_windspeed are bound to the end of the trip we will drop them, aswell as mean_trip_seconds, mean_trip_miles, mean_trip_total, as they would be data we wouldnt have if we tried to predict a future demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.drop(columns = ['mean_trip_seconds','mean_trip_miles','mean_trip_total','end_temp','end_precip','end_windspeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_amount</th>\n",
       "      <th>start_temp</th>\n",
       "      <th>start_precip</th>\n",
       "      <th>start_windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>423.985502</td>\n",
       "      <td>11.174750</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>7.092280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>313.461767</td>\n",
       "      <td>10.013487</td>\n",
       "      <td>0.231349</td>\n",
       "      <td>3.416467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>2.627500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>378.000000</td>\n",
       "      <td>10.905000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>639.000000</td>\n",
       "      <td>20.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1487.000000</td>\n",
       "      <td>29.980000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_amount   start_temp  start_precip  start_windspeed\n",
       "count  8760.000000  8760.000000   8760.000000      8760.000000\n",
       "mean    423.985502    11.174750      0.056735         7.092280\n",
       "std     313.461767    10.013487      0.231349         3.416467\n",
       "min       0.000000   -16.450000      0.000000         0.220000\n",
       "25%     146.000000     2.627500      0.000000         4.460000\n",
       "50%     378.000000    10.905000      0.000000         6.930000\n",
       "75%     639.000000    20.312500      0.000000         9.300000\n",
       "max    1487.000000    29.980000      1.000000        21.830000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.astype({'trip_start_timestamp': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and normalizing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start normalizing, we will first split our dataset. We will create a Test and training set, as well as a validation set, which we will test against cross validation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train & test data\n",
    "train_dataset = taxi.sample(frac=0.7, random_state=0)\n",
    "test_dataset = taxi.drop(train_dataset.index)\n",
    "#also we will split split train data into validation for later comparision vs cross validation\n",
    "train_vali_dataset = train_dataset.sample(frac=0.8, random_state=0)\n",
    "vali_dataset = train_dataset.drop(train_vali_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create datasets from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, batch_size=32):\n",
    "  df = dataframe.copy()\n",
    "  labels = df.pop('trip_amount')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((df, labels))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our target variable and all our features except the time axis are numerical and continous we will at the start ignore trip_start_timestamp. Also using time series data requires further work to be useable, which we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train_dataset.drop([\"trip_start_timestamp\"], axis=1))\n",
    "#test_ds = df_to_dataset(test_dataset.drop([\"trip_start_timestamp\"], axis=1))\n",
    "\n",
    "train_vali_ds = df_to_dataset(train_vali_dataset.drop([\"trip_start_timestamp\"], axis=1))\n",
    "vali_ds = df_to_dataset(vali_dataset.drop([\"trip_start_timestamp\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.01518524e+02 5.77008724e-02 1.17243452e+01]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#normalizing\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "feature_ds = train_ds.map(lambda x, y: x)\n",
    "normalizer.adapt(feature_ds)\n",
    "print(normalizer.variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the variance is roughly in line with the means we calculated earlier, we can assume that all our features have normalized correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compute our \"baseline\" prediction, using 2 Linear Layers with 64 Nodes and RELU Activation functions. It is very basic and therefore a good start. We also define a set learning rate, optimization and loss function and will only try to improve it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model(lr=0.01):\n",
    "  mod = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  opt = Adam(learning_rate=lr)\n",
    "  mod.compile(optimizer=opt,\n",
    "                loss='mse',\n",
    "                metrics=['mae', RSquare()])\n",
    "  return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction with validation split\n",
    "def split_valid(train_set, valid_set, Built_model, num_epochs = 30, verb_f = 0, verb_e = 0):\n",
    "    #higher epochs lead to better training but worse validation/testing results\n",
    "    Built_model.fit(train_set, epochs=num_epochs, verbose=verb_f)\n",
    "\n",
    "    val_mse, val_mae, val_r2 = Built_model.evaluate(valid_set, verbose=verb_e)\n",
    "    print(\"MSE\", val_mse)\n",
    "    print(\"MAE\", val_mae)\n",
    "    print(\"R2\", val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction with cross validation\n",
    "def cross_valid(ds, Built_model, shards = 5, num_epochs = 25, verb_f = 0, verb_e = 0 ):\n",
    "    #higher epochs lead to better training but worse validation/testing results\n",
    "    all_mse_scores = [] \n",
    "    all_mae_scores = [] \n",
    "    all_r2_scores = [] \n",
    "\n",
    "    for i in range(shards):\n",
    "        cross_vali_ds = ds.shard(num_shards=shards, index=i)\n",
    "        init = True\n",
    "\n",
    "        for j in range(shards):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if init:\n",
    "                cross_train_ds = ds.shard(num_shards=shards, index=j)\n",
    "                init = False\n",
    "                continue\n",
    "            cross_train_ds = cross_train_ds.concatenate(ds.shard(num_shards=shards, index=j))\n",
    "\n",
    "        \n",
    "        Built_model.fit(cross_train_ds, epochs=num_epochs, verbose=verb_f)\n",
    "\n",
    "        val_mse, val_mae, val_r2 = Built_model.evaluate(cross_vali_ds, verbose=verb_e)\n",
    "        \n",
    "        # Add Mean Absolut Error to All Scored List\n",
    "        all_mse_scores.append(val_mse)\n",
    "        all_mae_scores.append(val_mae)\n",
    "        all_r2_scores.append(val_r2)\n",
    "\n",
    "    print(\"Mean MSE\", np.mean(all_mse_scores))\n",
    "    print(\"Mean MAE\", np.mean(all_mae_scores))\n",
    "    print(\"Mean R2\", np.mean(all_r2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 915us/step - loss: 92081.8516 - mae: 240.6977 - r_square: 0.1068\n",
      "MSE 92081.8515625\n",
      "MAE 240.69772338867188\n",
      "R2 0.10678142309188843\n"
     ]
    }
   ],
   "source": [
    "model = get_basic_model()\n",
    "split_valid(train_vali_ds, vali_ds, model, verb_e = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with extremly little data our Accuracy is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 911us/step - loss: 84509.7656 - mae: 233.3097 - r_square: 0.1351\n",
      "39/39 [==============================] - 0s 898us/step - loss: 81695.3672 - mae: 228.7542 - r_square: 0.1593\n",
      "38/38 [==============================] - 0s 876us/step - loss: 81151.7109 - mae: 227.7116 - r_square: 0.1690\n",
      "38/38 [==============================] - 0s 896us/step - loss: 80641.1250 - mae: 226.0003 - r_square: 0.1803\n",
      "38/38 [==============================] - 0s 867us/step - loss: 77790.2344 - mae: 224.3345 - r_square: 0.1879\n",
      "Mean MSE 81157.640625\n",
      "Mean MAE 228.0220703125\n",
      "Mean R2 0.16630287170410157\n"
     ]
    }
   ],
   "source": [
    "model2 = get_basic_model()\n",
    "cross_valid(train_ds, model2, shards = 5, verb_e = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation results in better results, while split validation is way faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different model compositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test 3 other layer compositions to try to increase our R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE 78991.70625\n",
      "Mean MAE 224.50763854980468\n",
      "Mean R2 0.1885099768638611\n"
     ]
    }
   ],
   "source": [
    "model3 = get_basic_model()\n",
    "model3.pop()\n",
    "model3.pop()\n",
    "model3.add(layers.Dense(128, activation=\"relu\"))\n",
    "model3.add(layers.Dense(256, activation=\"relu\"))\n",
    "model3.add(layers.Dense(512, activation=\"relu\"))\n",
    "model3.add(layers.Dense(256, activation=\"relu\"))\n",
    "model3.add(layers.Dense(128, activation=\"relu\"))\n",
    "model3.add(layers.Dense(64, activation=\"relu\"))\n",
    "model3.add(layers.Dense(1))\n",
    "#split_valid(train_vali_ds, vali_ds, model3)\n",
    "cross_valid(train_ds, model3, shards = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE 80199.2890625\n",
      "Mean MAE 225.8953857421875\n",
      "Mean R2 0.17613511085510253\n"
     ]
    }
   ],
   "source": [
    "model4 = get_basic_model()\n",
    "model4.pop()\n",
    "model4.pop()\n",
    "model4.add(layers.Dense(18, activation=\"relu\"))\n",
    "model4.add(layers.Dense(9, activation=\"relu\"))\n",
    "model4.add(layers.Dense(18, activation=\"relu\"))\n",
    "model4.add(layers.Dense(1))\n",
    "#split_valid(train_vali_ds, vali_ds, model4)\n",
    "cross_valid(train_ds, model4, shards = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE 79054.7203125\n",
      "Mean MAE 224.33911743164063\n",
      "Mean R2 0.18783769607543946\n"
     ]
    }
   ],
   "source": [
    "model5 = get_basic_model()\n",
    "model5.pop()\n",
    "model5.pop()\n",
    "model5.add(layers.Dense(1024, activation=\"relu\"))\n",
    "model5.add(layers.Dense(512, activation=\"relu\"))\n",
    "model5.add(layers.Dense(256, activation=\"relu\"))\n",
    "model5.add(layers.Dense(1))\n",
    "#split_valid(train_vali_ds, vali_ds, model5)\n",
    "cross_valid(train_ds, model5, shards = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though more noded layers highly increase the computation time, the results are not improved significantly until the last \"high noded\" version, which also has a very high computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to improve our results by using precipation as a categorical and also extracting catgoricals from the time data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we will have to create multiple help functions. The first one creates  a dictonary dataset, since our features will not be of the same dtype going forward and tensors can't handle that . The second one takes a dict and stacks it to make it processable by a normaization layer. \n",
    "The third one is our precrocessing layer, in which we take our data and according to their datatype (given by lists of feature names), we will preprocess them. The last one takes a preprocessing layer and build a model with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use A special Crossvalidation technique for timeseries from sklearn TimeSeriesSplit. This allows us to predict Timeseries accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first one creates  a dictonary dataset, since our features will not be of the same dtype going forward and tensors can't handle that \n",
    "def df_to_dictonary_dataset(dataframe, batch_size=32, target = 'trip_demand'):\n",
    "  df = dataframe.copy()\n",
    "  labels = df.pop(target)\n",
    "  \n",
    "  dict_ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "  dict_ds = dict_ds.batch(batch_size)\n",
    "  dict_ds = dict_ds.prefetch(batch_size)\n",
    "  return dict_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second one takes a dict and stacks it to make it processable by a normaization layer.\n",
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The third one is our precrocessing layer, in which we take our data and according to their datatype (given by lists of feature names), we will preprocess them.\n",
    "def build_preprocessed(processing_df, bin_feat, cate_feat, num_feat):\n",
    "  inputs = {} #convert data into keras input\n",
    "  for name, column in processing_df.items():\n",
    "    if (name in cate_feat or name in bin_feat):\n",
    "      dtype = tf.int64\n",
    "    else:\n",
    "      dtype = tf.float32\n",
    "\n",
    "    inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "\n",
    "  #preprocess binary data -> not much preprocessing being done\n",
    "  preprocessed = []\n",
    "  for name in bin_feat:\n",
    "      inp = inputs[name]\n",
    "      inp = inp[:, tf.newaxis]\n",
    "      float_value = tf.cast(inp, tf.float32)\n",
    "      preprocessed.append(float_value)\n",
    "\n",
    "  #preprocess numeric data -> normization\n",
    "  numeric_features = pd.DataFrame()\n",
    "  for name in num_feat:\n",
    "    numeric_features[name]=processing_df[name]\n",
    "\n",
    "  tf.convert_to_tensor(numeric_features)\n",
    "\n",
    "  normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "  normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "  numeric_inputs = {}\n",
    "  for name in num_feat:\n",
    "    numeric_inputs[name]=inputs[name]\n",
    "\n",
    "  numeric_inputs = stack_dict(numeric_inputs)\n",
    "  numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "  preprocessed.append(numeric_normalized)\n",
    "\n",
    "  #preprocess categorical data -> create dummy binaries for each category of a feature\n",
    "  for name in cate_feat:\n",
    "    vocab = sorted(set(processing_df[name]))\n",
    "\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "    x = inputs[name][:, tf.newaxis]\n",
    "    x = lookup(x)\n",
    "    preprocessed.append(x)\n",
    "\n",
    "  return inputs, preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The last one takes a preprocessing layer and build a model with it.\n",
    "def do_preprocessing_and_model_building(processing_df, bin_feat, cate_feat, num_feat, lr = 0.1):\n",
    "  inputs, preprocessed = build_preprocessed(processing_df, bin_feat, cate_feat, num_feat)\n",
    "  preprocessed_result = tf.concat(preprocessed, axis=-1)\n",
    "  preprocessor = tf.keras.Model(inputs, preprocessed_result)\n",
    "\n",
    "  #Since our results in model5 are so high we will use their layers\n",
    "  \n",
    "  body = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  x = preprocessor(inputs)\n",
    "  result = body(x)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "  opt = Adam(#learning_rate=lr\n",
    "  )\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='mse',\n",
    "                metrics=['mae', RSquare()])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_and_cross_valid_for_timedata(df, binary_feature_names, categorical_feature_names, numeric_feature_names, splits = 5, num_epochs = 25, verb_f = 0, verb_e = 0, test = False, target = 'trip_demand', ler = 0.1):\n",
    "    work_df = df.copy()\n",
    "\n",
    "    work_df_preprocessing= work_df.loc[:, work_df.columns != target]\n",
    "    mod = do_preprocessing_and_model_building(work_df_preprocessing, binary_feature_names, categorical_feature_names, numeric_feature_names, lr = ler)\n",
    "\n",
    "        \n",
    "    all_mse_scores = [] \n",
    "    all_mae_scores = [] \n",
    "    all_r2_scores = [] \n",
    "    tscv = TimeSeriesSplit(splits)\n",
    "    counter = 0\n",
    "    #epoch_inc = ((num_epochs)/splits)\n",
    "    \n",
    "    for train_index, test_index in tscv.split(work_df):\n",
    "        counter = counter+1\n",
    "        #epochs= int(epoch_inc*counter) #epochs fitted to dataset size\n",
    "        \n",
    "        cross_train_df, cross_vali_df = work_df.iloc[train_index, :], work_df.iloc[test_index,:]\n",
    "        \n",
    "        cross_train_ds = df_to_dictonary_dataset(cross_train_df, target = target)\n",
    "        cross_vali_ds = df_to_dictonary_dataset(cross_vali_df, target = target)\n",
    "\n",
    "        if test and counter == splits:\n",
    "            val_mse, val_mae, val_r2 = mod.evaluate(cross_vali_ds, verbose=verb_e)\n",
    "            continue\n",
    "        elif test == False and counter == splits:\n",
    "            continue\n",
    "\n",
    "        mod.fit(cross_train_ds, epochs=num_epochs, verbose=verb_f)\n",
    "\n",
    "        val_mse, val_mae, val_r2 = mod.evaluate(cross_vali_ds, verbose=verb_e)\n",
    "        \n",
    "        # Add Mean Absolut Error to All Scored List\n",
    "        all_mse_scores.append(val_mse)\n",
    "        all_mae_scores.append(val_mae)\n",
    "        all_r2_scores.append(val_r2)\n",
    "    \n",
    "    \n",
    "    print(\"Mean MSE\", np.mean(all_mse_scores))\n",
    "    print(\"Mean MAE\", np.mean(all_mae_scores))\n",
    "    print(\"Mean R2\", np.mean(all_r2_scores))\n",
    "    if test:\n",
    "        print(\"\")\n",
    "        print(\"Test MSE\", val_mse)\n",
    "        print(\"Test MAE\", val_mae)\n",
    "        print(\"Test R2\", val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_for_timedata(df, bin_feat, num_feat, splits = 5, num_epochs = 25, verb_f = 0, verb_e = 0, test = False, target = 'trip_demand', lr = 0.01):\n",
    "    split_df = df.copy()\n",
    "    labels = split_df.pop(target)\n",
    "    inputs = {} #convert data into keras input\n",
    "    for name, column in split_df.items():\n",
    "        if (name in bin_feat):\n",
    "            dtype = tf.int64\n",
    "        else:\n",
    "            dtype = tf.float32\n",
    "\n",
    "        inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "\n",
    "    preprocessed = []\n",
    "    for name in bin_feat:\n",
    "        inp = inputs[name]\n",
    "        inp = inp[:, tf.newaxis]\n",
    "        float_value = tf.cast(inp, tf.float32)\n",
    "        preprocessed.append(float_value)\n",
    "\n",
    "    numeric_features = pd.DataFrame()\n",
    "    for name in num_feat:\n",
    "        numeric_features[name]=split_df[name]\n",
    "\n",
    "    tf.convert_to_tensor(numeric_features)\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "    numeric_inputs = {}\n",
    "    for name in num_feat:\n",
    "        numeric_inputs[name]=inputs[name]\n",
    "\n",
    "    numeric_inputs = stack_dict(numeric_inputs)\n",
    "    numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "    preprocessed.append(numeric_normalized)\n",
    "\n",
    "    preprocessor = tf.keras.Model(inputs, (tf.concat(preprocessed, axis=-1)))\n",
    "\n",
    "    body = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    result = body(preprocessor(inputs))\n",
    "    model = tf.keras.Model(inputs, result)\n",
    "    model.compile(optimizer=Adam(#learning_rate=lr\n",
    "                    ),\n",
    "                    loss='mse',\n",
    "                    metrics=['mae', RSquare()])\n",
    "\n",
    "    \n",
    "    all_mse_scores = [] \n",
    "    all_mae_scores = [] \n",
    "    all_r2_scores = [] \n",
    "    tscv = TimeSeriesSplit(splits)\n",
    "    counter = 0\n",
    "    #epoch_inc = ((num_epochs)/splits)\n",
    "    work_df = df.copy()\n",
    "    for train_index, test_index in tscv.split(work_df):\n",
    "        counter = counter+1\n",
    "        #epochs= int(epoch_inc*counter) #epochs fitted to dataset size\n",
    "        \n",
    "        cross_train_df, cross_vali_df = work_df.iloc[train_index, :], work_df.iloc[test_index,:]\n",
    "        \n",
    "        cross_train_ds = df_to_dictonary_dataset(cross_train_df, target = target)\n",
    "        cross_vali_ds = df_to_dictonary_dataset(cross_vali_df, target = target)\n",
    "\n",
    "        if test and counter == splits:\n",
    "            val_mse, val_mae, val_r2 = model.evaluate(cross_vali_ds, verbose=verb_e)\n",
    "            continue\n",
    "        elif test == False and counter == splits:\n",
    "            continue\n",
    "\n",
    "        model.fit(cross_train_ds, epochs=num_epochs, verbose=verb_f)\n",
    "\n",
    "        val_mse, val_mae, val_r2 = model.evaluate(cross_vali_ds, verbose=verb_e)\n",
    "        \n",
    "        # Add Mean Absolut Error to All Scored List\n",
    "        all_mse_scores.append(val_mse)\n",
    "        all_mae_scores.append(val_mae)\n",
    "        all_r2_scores.append(val_r2)\n",
    "    \n",
    "    \n",
    "    print(\"Mean MSE\", np.mean(all_mse_scores))\n",
    "    print(\"Mean MAE\", np.mean(all_mae_scores))\n",
    "    print(\"Mean R2\", np.mean(all_r2_scores))\n",
    "    if test:\n",
    "        print(\"\")\n",
    "        print(\"Test MSE\", val_mse)\n",
    "        print(\"Test MAE\", val_mae)\n",
    "        print(\"Test R2\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application for precip as categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply our preprocessing layer to our dataset with precipitation as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_prec = taxi.copy()\n",
    "taxi_prec = taxi_prec.drop([\"trip_start_timestamp\"], axis=1)\n",
    "taxi_prec[\"start_precip\"] = taxi_prec[\"start_precip\"].apply(lambda x: 0 if (x <= 0) else 1 )\n",
    "\n",
    "train_prec = taxi_prec.sample(frac=0.7, random_state=0)\n",
    "test_prec = taxi_prec.drop(train_prec.index)\n",
    "\n",
    "train_prec_ds = df_to_dictonary_dataset(train_prec, target = 'trip_amount')\n",
    "#test_prec_ds = df_to_dictonary_dataset(test_prec, target = 'trip_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE 81852.04375\n",
      "Mean MAE 231.20285034179688\n",
      "Mean R2 0.15918352603912353\n"
     ]
    }
   ],
   "source": [
    "binary_feature_names = ['start_precip']\n",
    "categorical_feature_names = []\n",
    "numeric_feature_names = ['start_temp', 'start_windspeed']\n",
    "\n",
    "taxi_prec_preprocessing= taxi_prec.loc[:, taxi_prec.columns != 'trip_amount']\n",
    "#The whole dataset is thrown into processing and model building but no fitting is done yet so that does not destroy the purpose of validation\n",
    "model6 = do_preprocessing_and_model_building(taxi_prec_preprocessing, binary_feature_names, categorical_feature_names, numeric_feature_names)\n",
    "\n",
    "cross_valid(train_prec_ds, model6, shards = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are roughly the same as when using precipation as a continous numerical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application for timedata as categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will will apply our Helper functions to categoricals from the timedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 1ms/step - loss: 11713.7256 - mae: 82.2333 - r_square: 0.6822\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 25198.5762 - mae: 115.3797 - r_square: 0.5622\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 33648.6016 - mae: 144.3593 - r_square: 0.5909\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 71746.4375 - mae: 214.9914 - r_square: 0.4347\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 103767.5156 - mae: 262.2021 - r_square: 0.2328\n",
      "Mean MSE 35576.835205078125\n",
      "Mean MAE 139.24091911315918\n",
      "Mean R2 0.567512720823288\n",
      "\n",
      "Test MSE 103767.515625\n",
      "Test MAE 262.2021179199219\n",
      "Test R2 0.23279625177383423\n"
     ]
    }
   ],
   "source": [
    "taxi_date = taxi.copy()\n",
    "taxi_date[\"weekday\"] = taxi_date[\"trip_start_timestamp\"].apply(lambda x:x.dayofweek)\n",
    "taxi_date[\"month\"] = taxi_date[\"trip_start_timestamp\"].apply(lambda x:x.month)\n",
    "taxi_date[\"is_weekend\"] = taxi_date[\"weekday\"].apply(lambda x: 1 if (x in [5,6]) else 0)\n",
    "taxi_date[\"hour\"] = taxi_date[\"trip_start_timestamp\"].apply(lambda x:x.hour)\n",
    "taxi_date.set_index('trip_start_timestamp', inplace=True)\n",
    "taxi_date.sort_index(inplace=True)\n",
    "\n",
    "binary_feature_names = ['is_weekend']\n",
    "categorical_feature_names = ['weekday', 'month', 'hour']\n",
    "numeric_feature_names = ['start_temp', 'start_precip', 'start_windspeed']\n",
    "\n",
    "split_data_and_cross_valid_for_timedata(taxi_date, binary_feature_names, categorical_feature_names, numeric_feature_names, splits = 5, verb_e = 1, num_epochs = 60, test = True, target = 'trip_amount')\n",
    "#even though our model is extremly overfitted, when using lower epochs the test results are even worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can't use sample since we dont want to split our timedata\n",
    "n = len(taxi_date)\n",
    "train_date = taxi_date[0:int(n*0.7)]\n",
    "test_date = taxi_date[int(n*0.7):]\n",
    "\n",
    "train_date_ds = df_to_dictonary_dataset(train_date, target = 'trip_amount')\n",
    "#test_date_ds = df_to_dictonary_dataset(test_date, target = 'trip_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 1ms/step - loss: 3220.5232 - mae: 41.2246 - r_square: 0.9462\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 2164.0095 - mae: 32.6322 - r_square: 0.9681\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1938.0339 - mae: 31.1401 - r_square: 0.9720\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1449.5778 - mae: 27.1895 - r_square: 0.9788\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1597.1178 - mae: 26.3157 - r_square: 0.9743\n",
      "Mean MSE 2073.85244140625\n",
      "Mean MAE 31.700415420532227\n",
      "Mean R2 0.9678538560867309\n"
     ]
    }
   ],
   "source": [
    "taxi_date_preprocessing= taxi_date.loc[:, taxi_date.columns != 'trip_amount']\n",
    "#The whole dataset is thrown into processing and model building but no fitting is done yet so that does not destroy the purpose of validation\n",
    "model7 = do_preprocessing_and_model_building(taxi_date_preprocessing, binary_feature_names, categorical_feature_names, numeric_feature_names)\n",
    "\n",
    "cross_valid(train_date_ds, model7, shards = 5, num_epochs = 20,  verb_e = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/83 [..............................] - ETA: 0s - loss: 14849.6270 - mae: 109.0818 - r_square: 0.8879"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 83865.6250 - mae: 232.4762 - r_square: 0.3819\n",
      "MSE 83865.625\n",
      "MAE 232.4762420654297\n",
      "R2 0.3818899393081665\n"
     ]
    }
   ],
   "source": [
    "test_date_ds = df_to_dictonary_dataset(test_date, target = 'trip_amount')\n",
    "val_mse, val_mae, val_r2 = model7.evaluate(test_date_ds)\n",
    "print(\"MSE\", val_mse)\n",
    "print(\"MAE\", val_mae)\n",
    "print(\"R2\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the categorical timedata included our results have improved significantly, but as we can see our Model is extremly overfitted. We will now introduce the aggregated time/geodata to help alleviate this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, model2, model3, model4, model5, model6, model7, vali_ds, vali_dataset, train_vali_ds, train_vali_dataset, train_prec_ds, train_prec, train_ds, train_date_ds, train_date, train_dataset\n",
    "del feature_ds, normalizer, taxi_date, taxi_prec, taxi_prec_preprocessing, taxi_date_preprocessing, test_dataset, test_date, test_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying found knowledge to Timebuckets & Geodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start to use the aggregated datasets (timebuckets & Geodata). For that we will first start with aggregation for hourly and a high resolution geoanalysis and then compare bigger timebuckets and different georesolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_feature_names = ['is_weekday', 'is_holiday', 'season_Autumn', 'season_Spring', 'season_Summer', 'season_Winter', 'precip']\n",
    "categorical_feature_names = []\n",
    "numeric_feature_names = ['temp_z', 'windspeed_z', 'temp_z_ma_7d', 'temp_z_std_7d', 'weekday_sin', 'weekday_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>trip_demand</th>\n",
       "      <th>temp_z</th>\n",
       "      <th>precip</th>\n",
       "      <th>windspeed_z</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>temp_z_ma_7d</th>\n",
       "      <th>temp_z_std_7d</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>season_Autumn</th>\n",
       "      <th>season_Spring</th>\n",
       "      <th>season_Summer</th>\n",
       "      <th>season_Winter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.703198e+10</th>\n",
       "      <th>2021-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.248857</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.217303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.053704</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.243863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.053704</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.246859</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.053704</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 03:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.231879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060771</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.053704</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 04:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.213902</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.053704</td>\n",
       "      <td>0.181925</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         trip_demand    temp_z  precip  \\\n",
       "pickup_census_tract datetime                                             \n",
       "1.703198e+10        2021-01-01 00:00:00            0 -1.248857       0   \n",
       "                    2021-01-01 01:00:00            0 -1.243863       0   \n",
       "                    2021-01-01 02:00:00            0 -1.246859       0   \n",
       "                    2021-01-01 03:00:00            0 -1.231879       0   \n",
       "                    2021-01-01 04:00:00            0 -1.213902       0   \n",
       "\n",
       "                                         windspeed_z  is_holiday  is_weekday  \\\n",
       "pickup_census_tract datetime                                                   \n",
       "1.703198e+10        2021-01-01 00:00:00    -0.217303           1           0   \n",
       "                    2021-01-01 01:00:00     0.008083           1           0   \n",
       "                    2021-01-01 02:00:00     0.113459           1           0   \n",
       "                    2021-01-01 03:00:00     0.060771           1           0   \n",
       "                    2021-01-01 04:00:00     0.069552           1           0   \n",
       "\n",
       "                                         temp_z_ma_7d  temp_z_std_7d  \\\n",
       "pickup_census_tract datetime                                           \n",
       "1.703198e+10        2021-01-01 00:00:00     -1.053704       0.181925   \n",
       "                    2021-01-01 01:00:00     -1.053704       0.181925   \n",
       "                    2021-01-01 02:00:00     -1.053704       0.181925   \n",
       "                    2021-01-01 03:00:00     -1.053704       0.181925   \n",
       "                    2021-01-01 04:00:00     -1.053704       0.181925   \n",
       "\n",
       "                                         weekday_sin  weekday_cos  \\\n",
       "pickup_census_tract datetime                                        \n",
       "1.703198e+10        2021-01-01 00:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 01:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 02:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 03:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 04:00:00    -0.866025         -0.5   \n",
       "\n",
       "                                         season_Autumn  season_Spring  \\\n",
       "pickup_census_tract datetime                                            \n",
       "1.703198e+10        2021-01-01 00:00:00              0              0   \n",
       "                    2021-01-01 01:00:00              0              0   \n",
       "                    2021-01-01 02:00:00              0              0   \n",
       "                    2021-01-01 03:00:00              0              0   \n",
       "                    2021-01-01 04:00:00              0              0   \n",
       "\n",
       "                                         season_Summer  season_Winter  \n",
       "pickup_census_tract datetime                                           \n",
       "1.703198e+10        2021-01-01 00:00:00              0              1  \n",
       "                    2021-01-01 01:00:00              0              1  \n",
       "                    2021-01-01 02:00:00              0              1  \n",
       "                    2021-01-01 03:00:00              0              1  \n",
       "                    2021-01-01 04:00:00              0              1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/\"\n",
    "Aggregated_Census_1H = pd.read_pickle(f\"{file_path}taxi_by_census_tract_1H.pkl\")\n",
    "Aggregated_Census_1H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "92555/92555 [==============================] - 116s 1ms/step - loss: 6.9376 - mae: 0.4414 - r_square: 0.4232\n",
      "Epoch 2/5\n",
      "92555/92555 [==============================] - 114s 1ms/step - loss: 9.9267 - mae: 0.5798 - r_square: 0.1746\n",
      "Epoch 3/5\n",
      "92555/92555 [==============================] - 115s 1ms/step - loss: 9.8523 - mae: 0.5744 - r_square: 0.1808\n",
      "Epoch 4/5\n",
      "92555/92555 [==============================] - 113s 1ms/step - loss: 9.7859 - mae: 0.5724 - r_square: 0.1863\n",
      "Epoch 5/5\n",
      "92555/92555 [==============================] - 114s 1ms/step - loss: 9.7471 - mae: 0.5707 - r_square: 0.1896\n",
      "26445/26445 [==============================] - 30s 1ms/step - loss: 0.0017 - mae: 0.0015 - r_square: -0.0011\n",
      "MSE 0.001681785681284964\n",
      "MAE 0.0015342546394094825\n",
      "R2 -0.0010836124420166016\n"
     ]
    }
   ],
   "source": [
    "n = len(Aggregated_Census_1H)\n",
    "train_agg = Aggregated_Census_1H[0:int(n*0.7)]\n",
    "vali_agg = Aggregated_Census_1H[int(n*0.7):int(n*0.9)]\n",
    "test_agg = Aggregated_Census_1H[int(n*0.9):]\n",
    "\n",
    "train_agg_preprocessing= Aggregated_Census_1H.loc[:, Aggregated_Census_1H.columns != 'trip_demand']\n",
    "\n",
    "model8 = do_preprocessing_and_model_building(train_agg_preprocessing, binary_feature_names, categorical_feature_names, numeric_feature_names)\n",
    "\n",
    "train_agg_ds = df_to_dictonary_dataset(train_agg, target = 'trip_demand')\n",
    "vali_agg_ds = df_to_dictonary_dataset(vali_agg, target = 'trip_demand')\n",
    "test_agg_ds = df_to_dictonary_dataset(test_agg, target = 'trip_demand')\n",
    "\n",
    "split_valid(train_agg_ds, vali_agg_ds, model8, num_epochs = 5, verb_f = 1, verb_e = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/13223 [..............................] - ETA: 5:42 - loss: 7.3649e-07 - mae: 8.5819e-04 - r_square: 0.0000e+00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13223/13223 [==============================] - 16s 1ms/step - loss: 0.0014 - mae: 0.0012 - r_square: -0.0010\n",
      "MSE 0.0014454586198553443\n",
      "MAE 0.001205719425342977\n",
      "R2 -0.0010412931442260742\n"
     ]
    }
   ],
   "source": [
    "val_mse, val_mae, val_r2 = model8.evaluate(test_agg_ds)\n",
    "print(\"MSE\", val_mse)\n",
    "print(\"MAE\", val_mae)\n",
    "print(\"R2\", val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "22037/22037 [==============================] - 28s 1ms/step - loss: 28.9026 - mae: 1.7564 - r_square: 0.4004\n",
      "Epoch 2/5\n",
      "22037/22037 [==============================] - 27s 1ms/step - loss: 29.4887 - mae: 1.8014 - r_square: 0.3882\n",
      "Epoch 3/5\n",
      "22037/22037 [==============================] - 28s 1ms/step - loss: 30.5781 - mae: 1.8612 - r_square: 0.3656\n",
      "Epoch 4/5\n",
      "22037/22037 [==============================] - 27s 1ms/step - loss: 31.5154 - mae: 1.9189 - r_square: 0.3462\n",
      "Epoch 5/5\n",
      "22037/22037 [==============================] - 28s 1ms/step - loss: 35.0685 - mae: 1.9932 - r_square: 0.2725\n",
      "22037/22037 [==============================] - 25s 1ms/step - loss: 0.5501 - mae: 0.1074 - r_square: -0.0123\n",
      "Epoch 1/5\n",
      "44074/44074 [==============================] - 57s 1ms/step - loss: 17.2382 - mae: 1.0766 - r_square: 0.3083\n",
      "Epoch 2/5\n",
      "44074/44074 [==============================] - 57s 1ms/step - loss: 16.6312 - mae: 1.0270 - r_square: 0.3326\n",
      "Epoch 3/5\n",
      "44074/44074 [==============================] - 57s 1ms/step - loss: 16.7904 - mae: 1.0321 - r_square: 0.3262\n",
      "Epoch 4/5\n",
      "44074/44074 [==============================] - 57s 1ms/step - loss: 16.7090 - mae: 1.0281 - r_square: 0.3295\n",
      "Epoch 5/5\n",
      "44074/44074 [==============================] - 57s 1ms/step - loss: 16.7560 - mae: 1.0279 - r_square: 0.3276\n",
      "22037/22037 [==============================] - 26s 1ms/step - loss: 0.0145 - mae: 0.0112 - r_square: -0.0050\n",
      "Epoch 1/5\n",
      "66111/66111 [==============================] - 87s 1ms/step - loss: 11.0359 - mae: 0.6764 - r_square: 0.3414\n",
      "Epoch 2/5\n",
      "66111/66111 [==============================] - 86s 1ms/step - loss: 12.5366 - mae: 0.7250 - r_square: 0.2518\n",
      "Epoch 3/5\n",
      "66111/66111 [==============================] - 86s 1ms/step - loss: 12.7089 - mae: 0.7284 - r_square: 0.2415\n",
      "Epoch 4/5\n",
      "66111/66111 [==============================] - 86s 1ms/step - loss: 12.6472 - mae: 0.7247 - r_square: 0.2452\n",
      "Epoch 5/5\n",
      "66111/66111 [==============================] - 86s 1ms/step - loss: 12.5945 - mae: 0.7215 - r_square: 0.2484\n",
      "22037/22037 [==============================] - 26s 1ms/step - loss: 0.0091 - mae: 0.0438 - r_square: -0.1969\n",
      "Epoch 1/5\n",
      "88148/88148 [==============================] - 116s 1ms/step - loss: 9.4131 - mae: 0.5412 - r_square: 0.2542\n",
      "Epoch 2/5\n",
      "88148/88148 [==============================] - 114s 1ms/step - loss: 9.3832 - mae: 0.5393 - r_square: 0.2565\n",
      "Epoch 3/5\n",
      "88148/88148 [==============================] - 115s 1ms/step - loss: 9.3560 - mae: 0.5377 - r_square: 0.2587\n",
      "Epoch 4/5\n",
      "88148/88148 [==============================] - 115s 1ms/step - loss: 9.3309 - mae: 0.5362 - r_square: 0.2607\n",
      "Epoch 5/5\n",
      "88148/88148 [==============================] - 114s 1ms/step - loss: 9.3077 - mae: 0.5348 - r_square: 0.2625\n",
      "22037/22037 [==============================] - 26s 1ms/step - loss: 0.0026 - mae: 0.0091 - r_square: -0.0323\n",
      "22037/22037 [==============================] - 26s 1ms/step - loss: 0.0013 - mae: 0.0084 - r_square: -0.0573\n",
      "Mean MSE 0.14407277229474857\n",
      "Mean MAE 0.042865036288276315\n",
      "Mean R2 -0.06162843108177185\n",
      "\n",
      "Test MSE 0.001310288324020803\n",
      "Test MAE 0.008437439799308777\n",
      "Test R2 -0.05730998516082764\n"
     ]
    }
   ],
   "source": [
    "cross_valid_for_timedata(Aggregated_Census_1H, binary_feature_names, numeric_feature_names, splits = 5, num_epochs = 5, verb_f = 1, verb_e = 1, test = True, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>trip_demand</th>\n",
       "      <th>temp_z</th>\n",
       "      <th>precip</th>\n",
       "      <th>windspeed_z</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>temp_z_ma_7d</th>\n",
       "      <th>temp_z_std_7d</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>season_Autumn</th>\n",
       "      <th>season_Spring</th>\n",
       "      <th>season_Summer</th>\n",
       "      <th>season_Winter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.703198e+10</th>\n",
       "      <th>2021-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.246887</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.05415</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.239893</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087530</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.05415</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 04:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.203924</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.05415</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 06:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.135484</td>\n",
       "      <td>0</td>\n",
       "      <td>0.865433</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.05415</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 08:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.103012</td>\n",
       "      <td>0</td>\n",
       "      <td>1.106598</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.05415</td>\n",
       "      <td>0.182006</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         trip_demand    temp_z  precip  \\\n",
       "pickup_census_tract datetime                                             \n",
       "1.703198e+10        2021-01-01 00:00:00            0 -1.246887       0   \n",
       "                    2021-01-01 02:00:00            0 -1.239893       0   \n",
       "                    2021-01-01 04:00:00            0 -1.203924       0   \n",
       "                    2021-01-01 06:00:00            0 -1.135484       0   \n",
       "                    2021-01-01 08:00:00            1 -1.103012       0   \n",
       "\n",
       "                                         windspeed_z  is_holiday  is_weekday  \\\n",
       "pickup_census_tract datetime                                                   \n",
       "1.703198e+10        2021-01-01 00:00:00    -0.105108           1           0   \n",
       "                    2021-01-01 02:00:00     0.087530           1           0   \n",
       "                    2021-01-01 04:00:00     0.190466           1           0   \n",
       "                    2021-01-01 06:00:00     0.865433           1           0   \n",
       "                    2021-01-01 08:00:00     1.106598           1           0   \n",
       "\n",
       "                                         temp_z_ma_7d  temp_z_std_7d  \\\n",
       "pickup_census_tract datetime                                           \n",
       "1.703198e+10        2021-01-01 00:00:00      -1.05415       0.182006   \n",
       "                    2021-01-01 02:00:00      -1.05415       0.182006   \n",
       "                    2021-01-01 04:00:00      -1.05415       0.182006   \n",
       "                    2021-01-01 06:00:00      -1.05415       0.182006   \n",
       "                    2021-01-01 08:00:00      -1.05415       0.182006   \n",
       "\n",
       "                                         weekday_sin  weekday_cos  \\\n",
       "pickup_census_tract datetime                                        \n",
       "1.703198e+10        2021-01-01 00:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 02:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 04:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 06:00:00    -0.866025         -0.5   \n",
       "                    2021-01-01 08:00:00    -0.866025         -0.5   \n",
       "\n",
       "                                         season_Autumn  season_Spring  \\\n",
       "pickup_census_tract datetime                                            \n",
       "1.703198e+10        2021-01-01 00:00:00              0              0   \n",
       "                    2021-01-01 02:00:00              0              0   \n",
       "                    2021-01-01 04:00:00              0              0   \n",
       "                    2021-01-01 06:00:00              0              0   \n",
       "                    2021-01-01 08:00:00              0              0   \n",
       "\n",
       "                                         season_Summer  season_Winter  \n",
       "pickup_census_tract datetime                                           \n",
       "1.703198e+10        2021-01-01 00:00:00              0              1  \n",
       "                    2021-01-01 02:00:00              0              1  \n",
       "                    2021-01-01 04:00:00              0              1  \n",
       "                    2021-01-01 06:00:00              0              1  \n",
       "                    2021-01-01 08:00:00              0              1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/\"\n",
    "Aggregated_Census_2H = pd.read_pickle(f\"{file_path}taxi_by_census_tract_2H.pkl\")\n",
    "Aggregated_Census_2H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "46278/46278 [==============================] - 60s 1ms/step - loss: 33.7246 - mae: 0.9411 - r_square: 0.2737\n",
      "Epoch 2/5\n",
      "46278/46278 [==============================] - 61s 1ms/step - loss: 39.2116 - mae: 1.0654 - r_square: 0.1555\n",
      "Epoch 3/5\n",
      "46278/46278 [==============================] - 57s 1ms/step - loss: 38.4806 - mae: 1.0597 - r_square: 0.1713\n",
      "Epoch 4/5\n",
      "46278/46278 [==============================] - 57s 1ms/step - loss: 38.6317 - mae: 1.1409 - r_square: 0.1680\n",
      "Epoch 5/5\n",
      "46278/46278 [==============================] - 57s 1ms/step - loss: 39.5945 - mae: 1.1488 - r_square: 0.1473\n",
      "13223/13223 [==============================] - 15s 1ms/step - loss: 0.0035 - mae: 0.0026 - r_square: -0.0017\n",
      "MSE 0.003525800770148635\n",
      "MAE 0.0025801549199968576\n",
      "R2 -0.0017242431640625\n"
     ]
    }
   ],
   "source": [
    "n = len(Aggregated_Census_2H)\n",
    "train_agg = Aggregated_Census_2H[0:int(n*0.7)]\n",
    "vali_agg = Aggregated_Census_2H[int(n*0.7):int(n*0.9)]\n",
    "test_agg = Aggregated_Census_2H[int(n*0.9):]\n",
    "\n",
    "train_agg_ds = df_to_dictonary_dataset(train_agg, target = 'trip_demand')\n",
    "vali_agg_ds = df_to_dictonary_dataset(vali_agg, target = 'trip_demand')\n",
    "test_agg_ds = df_to_dictonary_dataset(test_agg, target = 'trip_demand')\n",
    "\n",
    "train_agg_preprocessing= Aggregated_Census_2H.loc[:, Aggregated_Census_2H.columns != 'trip_demand']\n",
    "\n",
    "\n",
    "model9 = do_preprocessing_and_model_building(train_agg_preprocessing, binary_feature_names, categorical_feature_names, numeric_feature_names)\n",
    "\n",
    "split_valid(train_agg_ds, vali_agg_ds, model9, num_epochs = 5, verb_f = 1, verb_e = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6612/6612 [==============================] - 8s 1ms/step - loss: 0.0031 - mae: 0.0019 - r_square: -0.0012\n",
      "MSE 0.0030616074800491333\n",
      "MAE 0.0019228679593652487\n",
      "R2 -0.001232743263244629\n"
     ]
    }
   ],
   "source": [
    "val_mse, val_mae, val_r2 = model9.evaluate(test_agg_ds)\n",
    "print(\"MSE\", val_mse)\n",
    "print(\"MAE\", val_mae)\n",
    "print(\"R2\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mein Model einfach sehr fragw√ºrdige Ergebnisse liefert wird es gescrapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare with SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
